{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = r'\\\\aws-model04\\e$\\soundcast_root\\src\\2050'\n",
    "output_path = 'S:\\\\angela\\job_housing\\soundcast_2050'\n",
    "parcel_file_name = 'inputs\\\\accessibility\\\\parcels_urbansim.txt'\n",
    "geo_file_name = 'inputs\\\\accessibility\\\\parcels_suzanne.csv'\n",
    "nodes_file_name = 'inputs\\\\accessibility\\\\all_streets_nodes_2014.csv'\n",
    "links_file_name = 'inputs\\\\accessibility\\\\all_streets_links_2014.csv'\n",
    "#transit_stop_name = 'inputs\\\\accessibility\\\\transit_stops_2014.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inro.emme.database.emmebank as _eb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandana as pdna\n",
    "import os\n",
    "import re \n",
    "import sys\n",
    "from pyproj import Proj, transform\n",
    "sys.path.append(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "year = 2050\n",
    "geo = 'region'\n",
    "transit_time_max = 60\n",
    "bank_tod = '7to8'\n",
    "res_name_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distances = { # in meters; \n",
    "              # keys correspond to suffices of the resulting parcel columns\n",
    "              # ORIGINAL VALUES in feet!!\n",
    "             1: 2640, # 0.5 mile\n",
    "             #2: 5280, # 1 mile\n",
    "             3: 15840 # 3 miles\n",
    "             }\n",
    "geo_boundry = {'county': 'county_id',\n",
    "               'city': 'city_id', \n",
    "               'taz': 'TAZ_P',\n",
    "               'region': 'region_id'}\n",
    "parcel_attributes = {\n",
    "              \"sum\": ['EMPTOT_P'],\n",
    "              #\"ave\": [ \"PPRICDYP\", \"PPRICHRP\"]\n",
    "              }\n",
    "\n",
    "'''      \n",
    "parcel_attributes = {\n",
    "              \"sum\": [\"HH_P\", \"STUGRD_P\", \"STUHGH_P\", \"STUUNI_P\", \n",
    "                      \"EMPMED_P\", \"EMPOFC_P\", \"EMPEDU_P\", \"EMPFOO_P\", \"EMPGOV_P\", \"EMPIND_P\", \n",
    "                      \"EMPSVC_P\", \"EMPOTH_P\", \"EMPTOT_P\", \"EMPRET_P\",\n",
    "                      \"PARKDY_P\", \"PARKHR_P\", \"NPARKS\", \"APARKS\", \"daily_weighted_spaces\", \"hourly_weighted_spaces\"],\n",
    "              \"ave\": [ \"PPRICDYP\", \"PPRICHRP\"],\n",
    "              }\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get transit information, including walking to the bus, waiting, transferring, time on the transit vehicle\n",
    "def get_transit_information(bank):\n",
    "    bus_time = bank.matrix('auxwa').get_numpy_data() + bank.matrix('twtwa').get_numpy_data() + bank.matrix('ivtwa').get_numpy_data() \n",
    "    rail_time = bank.matrix('auxwr').get_numpy_data() + bank.matrix('twtwr').get_numpy_data() + bank.matrix('ivtwr').get_numpy_data() \n",
    "    transit_time = np.minimum(bus_time, rail_time)\n",
    "    transit_time = transit_time[0:3700, 0:3700]\n",
    "    transit_time_df = pd.DataFrame(transit_time)\n",
    "    transit_time_df['from'] = transit_time_df.index\n",
    "    transit_time_df = pd.melt(transit_time_df, id_vars= 'from', value_vars=list(transit_time_df.columns[0:3700]), var_name = 'to', value_name='transit_time')\n",
    "    # add 1 into zone id before join with parcel data\n",
    "    transit_time_df['to'] = transit_time_df['to'] + 1 \n",
    "    transit_time_df['from'] = transit_time_df['from'] + 1\n",
    "    return transit_time_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_transit_attribute(transit_time_data, transit_time_max, parcel_taz_id, transit_taz_id, attr_list, origin_df, dest_df):\n",
    "    # get destination information\n",
    "    transit = transit_time_data[transit_time_data.transit_time <= transit_time_max]\n",
    "    transit = transit.merge(dest_df, how = 'left', left_on = 'to', right_on = parcel_taz_id)\n",
    "    # groupby destination information by origin TAZ id \n",
    "    transit_emp = pd.DataFrame(transit.groupby(transit_taz_id)[attr_list].sum())\n",
    "    print transit_emp.head()\n",
    "    transit_emp.reset_index(inplace=True)\n",
    "    print transit_emp.head()\n",
    "    transit_df = pd.merge(transit_emp, origin_df, left_on = transit_taz_id, right_on = parcel_taz_id)\n",
    "    print transit_df.head()\n",
    "    return transit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_nodes_to_dataset(dataset, network, column_name, x_name, y_name):\n",
    "    \"\"\"Adds an attribute node_ids to the given dataset.\"\"\"\n",
    "    dataset[column_name] = network.get_node_ids(dataset[x_name].values, dataset[y_name].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_net_attribute(network, attr, fun):\n",
    "    print \"Processing %s\" % attr\n",
    "    newdf = None\n",
    "    for dist_index, dist in distances.iteritems():        \n",
    "        res_name = \"%s_%s\" % (re.sub(\"_?p$\", \"\", attr), dist_index) # remove '_p' if present\n",
    "        print res_name\n",
    "        res_name_list.append(res_name)\n",
    "        aggr = network.aggregate(dist, type=fun, decay=\"flat\", name=attr)\n",
    "        if newdf is None:\n",
    "            newdf = pd.DataFrame({res_name: aggr, \"node_ids\": aggr.index.values})\n",
    "        else:\n",
    "            newdf[res_name] = aggr\n",
    "    return newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get household weighted/averaged jobs \n",
    "def get_weighted_jobs(df, res_name_list):\n",
    "    for res_name in res_name_list:\n",
    "          weighted_res_name = 'HHweighted_' + res_name\n",
    "          df[weighted_res_name] = df[res_name]*df['HH_P']\n",
    "          print weighted_res_name\n",
    "    print df.head()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_jobs(df, geo, res_name_list):\n",
    "    df_groupby = df.groupby([geo]).sum()\n",
    "    df_groupby.reset_index(inplace = True)\n",
    "    for res_name in res_name_list: \n",
    "         weighted_res_name = 'HHweighted_' + res_name\n",
    "         averaged_res_name = 'HHaveraged_' + res_name\n",
    "         df_groupby[averaged_res_name] = df_groupby[weighted_res_name]/df_groupby['HH_P']\n",
    "    print df_groupby.head()\n",
    "    return df_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_missing_data(df, col):\n",
    "    #check for missing data!\n",
    "    for col_name in df.columns:\n",
    "        # daysim does not use EMPRSC_P\n",
    "        if col_name <> col:\n",
    "            if df[col_name].sum() == 0:\n",
    "                print col_name + ' column sum is zero! Exiting program.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "if you want to compute jobs within XXX transit time, \n",
    "please only run the code within this box\n",
    "'''\n",
    "\n",
    "output_file_name = geo + '_transit_' + str(year) + '_' + str(transit_time_max) + '_' + 'min.csv'\n",
    "\n",
    "# read data\n",
    "parcel_df = pd.read_csv(os.path.join(output_path, parcel_file_name), sep = ' ')\n",
    "geo_df = pd.DataFrame.from_csv(os.path.join(output_path, geo_file_name), sep = ',', index_col = None )\n",
    "check_missing_data(parcel_df, 'EMPRSC_P')\n",
    "\n",
    "# organize origin TAZ information\n",
    "geo_df = pd.merge(parcel_df, geo_df, left_on = 'PARCELID', right_on = 'parcel_id')\n",
    "city_dict = geo_df.set_index(['TAZ_P']).to_dict()['city_id']\n",
    "county_dict = geo_df.set_index(['TAZ_P']).to_dict()['county_id']\n",
    "city_name_dict = geo_df.set_index(['TAZ_P']).to_dict()['city_name']\n",
    "origin_df = pd.DataFrame(geo_df.groupby(['TAZ_P'])['HH_P'].sum())\n",
    "origin_df.reset_index(inplace=True)\n",
    "origin_df['city_id'] = origin_df['TAZ_P'].map(city_dict)\n",
    "origin_df['county_id'] = origin_df['TAZ_P'].map(county_dict)\n",
    "origin_df['city_name'] = origin_df['TAZ_P'].map(city_name_dict)\n",
    "origin_df['region_id'] = 1\n",
    "\n",
    "# orgnize destination TAZ information\n",
    "parcel_attributes_list = parcel_attributes['sum']\n",
    "print parcel_attributes_list\n",
    "dest_df = pd.DataFrame(geo_df.groupby(['TAZ_P'])[parcel_attributes_list].sum())\n",
    "dest_df.reset_index(inplace=True)\n",
    "\n",
    "# process transit time\n",
    "bank = _eb.Emmebank(os.path.join(model_path, 'Banks', bank_tod, 'emmebank'))\n",
    "transit_time_df = get_transit_information(bank)\n",
    "transit_df = process_transit_attribute(transit_time_df, transit_time_max, 'TAZ_P', 'from', parcel_attributes_list, origin_df, dest_df)\n",
    "\n",
    "# jobs by household \n",
    "weighted_jobs_df = get_weighted_jobs(transit_df, parcel_attributes_list)\n",
    "average_jobs_df = get_average_jobs(weighted_jobs_df, geo_boundry[geo], parcel_attributes_list) \n",
    "\n",
    "average_jobs_df.to_csv(os.path.join(output_path, output_file_name), index=False)\n",
    "print 'transit done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "if you want to compute jobs within XXX walk/bike distances, \n",
    "please only run the code within this box\n",
    "'''\n",
    "\n",
    "output_file_name = geo + str(year) +'_0.5_3_miles.csv'  \n",
    "\n",
    "# read data\n",
    "parcel_df = pd.read_csv(os.path.join(output_path, parcel_file_name), sep = ' ')\n",
    "nodes = pd.DataFrame.from_csv(os.path.join(output_path, nodes_file_name), sep = ',')\n",
    "links = pd.DataFrame.from_csv(os.path.join(output_path, links_file_name), sep = ',', index_col = None )\n",
    "geo_df = pd.DataFrame.from_csv(os.path.join(output_path, geo_file_name), sep = ',', index_col = None )\n",
    "check_missing_data(parcel_df, 'EMPRSC_P')\n",
    "\n",
    "new_parcel_df = pd.merge(parcel_df, geo_df, left_on = 'PARCELID', right_on='parcel_id', how = 'left')\n",
    "new_parcel_df['region_id'] = 1\n",
    "\n",
    "# assign impedance\n",
    "imp = pd.DataFrame(links.Shape_Length)\n",
    "imp = imp.rename(columns = {'Shape_Length':'distance'})\n",
    "# create pandana network\n",
    "net = pdna.network.Network(nodes.x, nodes.y, links.from_node_id, links.to_node_id, imp)\n",
    "\n",
    "for dist in distances:\n",
    "    print dist\n",
    "    net.precompute(dist)\n",
    "\n",
    "# assign network nodes to parcels, for buffer variables\n",
    "assign_nodes_to_dataset(new_parcel_df, net, 'node_ids', 'XCOORD_P', 'YCOORD_P')\n",
    "x, y = new_parcel_df.XCOORD_P, new_parcel_df.YCOORD_P\n",
    "new_parcel_df['node_ids'] = net.get_node_ids(x, y)\n",
    "\n",
    "# start processing attributes\n",
    "newdf = None\n",
    "for fun, attrs in parcel_attributes.iteritems():    \n",
    "    for attr in attrs:\n",
    "        net.set(new_parcel_df.node_ids, variable=parcel_df[attr], name=attr)    \n",
    "        res = process_net_attribute(net, attr, fun)\n",
    "        if newdf is None:\n",
    "            newdf = res\n",
    "        else:\n",
    "            newdf = pd.merge(newdf, res, on=\"node_ids\", copy=False)\n",
    "print res_name_list\n",
    "\n",
    "# jobs by household\n",
    "parcel_net_df = pd.merge(newdf, new_parcel_df[['node_ids', 'HH_P', geo_boundry[geo]]], on=\"node_ids\", copy=False)\n",
    "parcel_net_df = get_weighted_jobs(parcel_net_df, res_name_list)  \n",
    "parcel_net_df_groupby = get_average_jobs(parcel_net_df, geo_boundry[geo], res_name_list)\n",
    "\n",
    "\n",
    "parcel_net_df_groupby.to_csv(os.path.join(output_path, output_file_name), index=False)\n",
    "print 'walk/bike done'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
